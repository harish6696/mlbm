{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from hydra.utils import to_absolute_path\n",
    "#append the datasets starting with the word velocity froom  the folder \"final_dataset_for_train\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For raw feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_name = \"mixed\"\n",
    "num_channels = 3\n",
    "\n",
    "#order of raw fields: density, velocity_x, velocity_y\n",
    "\n",
    "base_folder = \"Datasets/final_combined_dataset_for_train\"\n",
    "Re_list = [100, 200, 300, 400, 500]\n",
    "data = torch.empty([0,num_channels,1024,256])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for Re in Re_list:\n",
    "    data_file = f\"{field_name}_Re_{Re}.pt\"\n",
    "    #join the path with the base folder\n",
    "    data_file_path = Path(base_folder).joinpath(data_file)\n",
    "    loaded_data = torch.load(data_file_path)\n",
    "    loaded_data = loaded_data[350:]\n",
    "    data = torch.cat([data, loaded_data], dim=0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.shape)\n",
    "#data = data[:,:,24:,:]  #uncomment this line to remove the top 24 rows (for 1000x256)\n",
    "print(data.shape)\n",
    "data = data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.mean(data, axis = (0,2,3))\n",
    "std= np.std(data, axis = (0,2,3))\n",
    "#print the field name and the mean value\n",
    "print(f\"{field_name}: {mean} and {std}\")\n",
    "#calculate the mean and std of the Re_list\n",
    "Re_mean = np.mean(np.array(Re_list))\n",
    "Re_std = np.std(np.array(Re_list))\n",
    "print(f\"Re: mean: {Re_mean} and std: {Re_std}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For derivative feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_data = torch.empty([0,num_channels,1024,256])\n",
    "for Re in Re_list:\n",
    "    data_file = f\"{field_name}_Re_{Re}.pt\"\n",
    "    #join the path with the base folder\n",
    "    data_file_path = Path(base_folder).joinpath(data_file)\n",
    "    loaded_data = torch.load(data_file_path)\n",
    "    loaded_data = loaded_data[350:]\n",
    "    loaded_data_acc = loaded_data[1:] - loaded_data[:-1]\n",
    "    print(loaded_data_acc.shape)\n",
    "\n",
    "    acc_data = torch.cat([acc_data, loaded_data_acc], dim=0) #accumulate the acc data\n",
    "\n",
    "print(acc_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create acceleration by taking the difference between two consecutive data points along dimension 0\n",
    "#acc = data[1:] - data[:-1]\n",
    "#print(acc.shape)\n",
    "print(acc_data.shape)\n",
    "#acc_data = acc_data[:,:,24:,:] #uncomment this line to remove the top 24 rows (for 1000x256)\n",
    "print(acc_data.shape)\n",
    "acc_data = acc_data.numpy()\n",
    "mean_acc = np.mean(acc_data, axis = (0,2,3))\n",
    "std_acc= np.std(acc_data, axis = (0,2,3))\n",
    "#print the field name and the mean value\n",
    "print(f\"{field_name}: {mean_acc} and {std_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raw data for Res 1000 x 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "data.shape (1953, 3, 1000, 256), we accumulate from timestep 350 to 1000 which is 651 steps per Re mixed: \n",
    "\n",
    "For Re from 200 to 400, the mean and std of the velocity field are for grid resolution 1000x256:\n",
    "velocity: \n",
    "    mean: [2.9989555e-02 -2.6079073e-07 (approx 0)]   \n",
    "    std:  [0.01385313  0.01032332]  \n",
    "\n",
    "density:\n",
    "    mean: 1.1302669\n",
    "    std: 0.05822831\n",
    "\n",
    "Re: mean: 300.0\n",
    "    std: 81.64965809277261\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "data.shape (3255, 3, 1000, 256), we accumulate from timestep 350 to 1000 which is 651 steps per Re, here we have 5 Re's in the dataset, 5*651 = 3255\n",
    "\n",
    "For Re from 100 to 500, the mean and std of the velocity field are for grid resolution 1000x256:\n",
    "velocity: \n",
    "    mean: [2.9990351e-02 2.3270729e-07 (approx 0)] \n",
    "    std: [0.01848454 0.01129391] \n",
    "\n",
    "density:\n",
    "    mean: 1.1165854\n",
    "    std: 0.05635629\n",
    "\n",
    "Re: mean: 300.0\n",
    "    std: 141.4213562373095\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Derivative (acceleration) velocity and density for res 1000 x 256 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "acc_data.shape: (1950, 3, 1000, 256), original data has 1953 samples in time. We accumulate from timestep 350 to 1000 which is 651 steps per Re mixed, then we take the differnce between two consecutive data points along dimension 0 to get the acceleration field, 651-->650 samples per Re, and we have 3 Re's in the dataset, 3*650 = 1950\n",
    "\n",
    "For Re from 200 to 400, the mean and std of the acceleration field are for grid resolution 1000x256: \n",
    "velocity: \n",
    "    mean: [5.8065979e-08 3.8627491e-07]  \n",
    "    std:  0.00771821 0.01142375\n",
    "\n",
    "density:\n",
    "    mean: 2.0737645e-04 \n",
    "    std:  0.00451678\n",
    "\n",
    "Re: mean: 300.0\n",
    "    std: 81.64965809277261\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "acc_data.shape: (3250, 3, 1000, 256), original data has 3255 samples in time. We accumulate from timestep 350 to 1000 which is 651 steps per Re mixed, then we take the differnce between two consecutive data points along dimension 0 to get the acceleration field, 651-->650 samples per Re, and we have 5 Re's in the dataset, 5*650 = 3250\n",
    "\n",
    "For Re from 100 to 500, the mean and std of the acceleration field are for grid resolution 1000x256: \n",
    "velocity: \n",
    "    mean: [3.0042081e-08 2.5793506e-07] \n",
    "    std:  [0.00977171 0.01426931]\n",
    "\n",
    "density:\n",
    "    mean: 1.8413871e-04\n",
    "    std: 0.00416971\n",
    "\n",
    "Re: mean: 300.0\n",
    "    std:  141.4213562373095\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stats for Raw-Data for res 1024 x 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "For Re from 200 to 400, the mean and std of the velocity field are for grid resolution 1024x256: \n",
    "\n",
    "data.shape (1953, 3, 1024, 256), we accumulate from timestep 350 to 1000 which is 651 steps per Re mixed. Here we have 3 Re's in the dataset, 3*651 = 1953\n",
    "velocity: \n",
    "    mean:   [ 2.9988546e-02 -1.5189227e-07 (approx 0)]\n",
    "    std:    [0.0137524  0.01020263] \n",
    "\n",
    "density:\n",
    "    mean: [1.1303548] \n",
    "    std: [0.05823061]\n",
    "\n",
    "Re: mean: 300.0\n",
    "    std: 81.64965809277261\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "For Re from 100 to 500, the mean and std of the velocity field are for grid resolution 1024x256:\n",
    "velocity: \n",
    "    mean:  [2.9989550e-02 2.9060382e-07 (approx 0)] \n",
    "    std:   [0.01839873 0.01116166]\n",
    "\n",
    "density:\n",
    "    mean:[1.1166793] \n",
    "    std: [0.05635419]\n",
    "\n",
    "Re: mean: 300.0\n",
    "    std: 141.4213562373095\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For Derivative(acceleration) of density and velocity for res 1024 x 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "acc_data.shape (1950, 3, 1024, 256), original data has 1953 samples in time. We accumulate from timestep 350 to 1000 which is 651 steps per Re mixed, then we take the differnce between two consecutive data points along dimension 0 to get the acceleration field, 651-->650 samples per Re, and we have 3 Re's in the dataset, 3*650 = 1950\n",
    "For Re from 200 to 400, the mean and std of the acceleration field are for grid resolution 1024x256:  \n",
    "velocity: \n",
    "    mean: [ 5.7614791e-08 3.7017534e-07]   \n",
    "    std: [ 0.00762723 0.0112895 ]   \n",
    "\n",
    "density:\n",
    "    mean: 2.0734874e-04\n",
    "    std: 0.00449273\n",
    "\n",
    "Re: mean: 300.0\n",
    "    std: 81.64965809277261\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "For Re from 100 to 500, the mean and std of the acceleration field are for grid resolution 1024x256: mixed:  and \n",
    "velocity: \n",
    "    mean: [ 2.9582768e-08 2.4923469e-07]  \n",
    "    std: [ 0.00965654 0.01410133]    \n",
    "\n",
    "density:\n",
    "    mean: 1.8413067e-04\n",
    "    std: 0.00414769\n",
    "\n",
    "Re: mean: 300.0\n",
    "    std: 141.4213562373095\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lbm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
